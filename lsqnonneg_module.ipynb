{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date:2018.07.21\n",
    "# Author: Runyu Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.function import once_differentiable\n",
    "from scipy.optimize import nnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LsqNonnegF(torch.autograd.Function):\n",
    "    '''\n",
    "    Define the forward and backward process for q(X,A) = argmin_{S >= 0} ||X - AS||_F^2\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, A):\n",
    "        # note that here the input X have size(n,m)\n",
    "        # output should have size(n, k)\n",
    "        # A should have size(k,m) and what we are actually doing is:\n",
    "        # min_{S >= 0} ||X - S*A||_F^2\n",
    "        # output[i,:] = argmin_{s >= 0} ||X[i,:] - s*A||_F^2 \n",
    "        # this is slightly different from what we do in NMF\n",
    "        [output, res] = lsqnonneg_tensor_version(A.data.t(), input.data.t())\n",
    "        # normalize the output\n",
    "        #output_sum = torch.sum(output, dim =1) + 1e-10\n",
    "        #output = output.t()/output_sum\n",
    "        #output = output.t()\n",
    "        #A.data = A.data.t()*output_sum\n",
    "        #A.data = A.data.t()\n",
    "        ctx.save_for_backward(input, A)\n",
    "        output = output.t()\n",
    "        ctx.intermediate = output\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    @once_differentiable # don't know if this is needed, it seems like if without this line then in backprop all the operations should be differentiable\n",
    "    def backward(ctx, grad_output):\n",
    "        input, A = ctx.saved_tensors\n",
    "        grad_input = grad_A = None\n",
    "        output = ctx.intermediate\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = calc_grad_X(grad_output.t(), A.t().data, output.t())# calculate gradient with respect to X\n",
    "            grad_input = grad_input.t()\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_A = calc_grad_A(grad_output.t(), A.t().data, output.t(), input.t().data) # calculate gradient with respect to A\n",
    "            grad_A = grad_A.t()\n",
    "        return grad_input, grad_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsqnonneg_tensor_version(C, D):\n",
    "    C = C.numpy() # Transforming to numpy array size(n,k)\n",
    "    D = D.numpy() # size(n,m)\n",
    "    n = D.shape[0]\n",
    "    m = D.shape[1]\n",
    "    k = C.shape[1]\n",
    "    X = np.zeros([k,m])\n",
    "    res_total = 0\n",
    "    for i in range(m):\n",
    "        d = D[:,i]\n",
    "        [x, res] = nnls(C, d)\n",
    "        res_total += res\n",
    "        X[:,i] = x\n",
    "    X = torch.from_numpy(X).double() # Transforming to torch Tensor\n",
    "    return X, res_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_X(grad_S, A, S):\n",
    "    A_np = A.numpy()\n",
    "    S_np = S.numpy()\n",
    "    grad_S_np = grad_S.numpy()\n",
    "    n = A.shape[0]\n",
    "    k = A.shape[1]\n",
    "    m = S.shape[1]\n",
    "    grad_X = np.zeros([n,m])\n",
    "    for i in range(m):\n",
    "        s = S_np[:,i]\n",
    "        supp = s!=0\n",
    "        grad_s_supp = grad_S_np[supp,i]\n",
    "        A_supp = A_np[:,supp]\n",
    "        grad_X[:,i] = np.linalg.pinv(A_supp).T@grad_s_supp\n",
    "    grad_X = torch.from_numpy(grad_X).double()\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_A(grad_S, A, S, X):\n",
    "    A_np = A.numpy()\n",
    "    S_np = S.numpy()\n",
    "    grad_S_np = grad_S.numpy()\n",
    "    X_np = X.numpy()\n",
    "    n = A.shape[0]\n",
    "    k = A.shape[1]\n",
    "    m = S.shape[1]\n",
    "    grad_A = np.zeros([n,k])\n",
    "    for l in range(m):\n",
    "        s = S_np[:,l]\n",
    "        supp = s!=0\n",
    "        A_supp = A_np[:,supp]\n",
    "        grad_s_supp = grad_S_np[supp,l:l+1]\n",
    "        x = X_np[:,l:l+1]\n",
    "        A_supp_inv = np.linalg.pinv(A_supp)\n",
    "        part1 = -(A_supp_inv.T@grad_s_supp)@(x.T@A_supp_inv.T)\n",
    "        part2 = (x - A_supp@(A_supp_inv@x))@((grad_s_supp.T@A_supp_inv)@A_supp_inv.T)\n",
    "        grad_A[:,supp] += part1 + part2\n",
    "    grad_A = torch.from_numpy(grad_A).double()\n",
    "    return grad_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LsqNonneg(nn.Module):\n",
    "    '''\n",
    "    Defining a submodule 'LsqNonneg' of the nn.Module\n",
    "    with network parameter: self.A which correspond to the A matrix in the NMF decomposition\n",
    "    '''\n",
    "    def __init__(self, m, k, initial_A = None):\n",
    "        super(LsqNonneg, self).__init__()\n",
    "        self.m = m;\n",
    "        self.k = k;\n",
    "        self.A = nn.Parameter(torch.DoubleTensor(k,m))\n",
    "        if initial_A is None:\n",
    "            self.A.data = torch.abs(torch.randn(k,m,dtype = torch.double)) # initialize the network parameter\n",
    "        else:\n",
    "            self.A.data = initial_A\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return LsqNonnegF.apply(input, self.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10\n",
    "# m = 10\n",
    "# k = 5\n",
    "# X_tensor = torch.randn(n,m, dtype = torch.double)\n",
    "# A_tensor = torch.randn(k,m, dtype = torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = Variable(X_tensor, requires_grad=True)\n",
    "# A = Variable(A_tensor, requires_grad = True)\n",
    "# X = torch.abs(X)\n",
    "# A = torch.abs(A)\n",
    "# input = (X, A)\n",
    "# test = gradcheck(LsqNonnegF().apply, input, eps = 1e-6, atol = 0, rtol = 1e-9)\n",
    "# print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
