{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 2018.07.22\n",
    "# Author: Runyu Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from deep_nmf.ipynb\n",
      "importing Jupyter notebook from lsqnonneg_module.ipynb\n",
      "importing Jupyter notebook from writer.ipynb\n"
     ]
    }
   ],
   "source": [
    "import Ipynb_importer\n",
    "from deep_nmf import Deep_NMF, Energy_Loss_Func, Fro_Norm\n",
    "from writer import Writer\n",
    "import torch\n",
    "import numpy as np\n",
    "from lsqnonneg_module import LsqNonneg\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unsupervised(net, X, loss_func, epoch = 10, lr = 1e-3, weight_decay = 1):\n",
    "    '''\n",
    "    ----- Discription\n",
    "    Training the unsupervised Deep_NMF with projection gradient descent\n",
    "    ----- Inputs:\n",
    "    net: A Deep_NMF object, note that it should be the unsupervised version, so c = None for the Deep_NMF.\n",
    "    X: The data matrix.\n",
    "    loss_func: The loss function, should be a Energy_Loss_func object, with lambd = None or 0\n",
    "    epoch: How many time you want to feed in the data matrix to the network, default 10\n",
    "    lr: learning rate, default 1e-3\n",
    "    weight_decay: the weight decay parameter, doing lr = lr*weight_decay every epoch\n",
    "    '''\n",
    "    history = Writer() # creating a Writer object to record the history for the training process\n",
    "    for i in range(epoch):\n",
    "        net.zero_grad()\n",
    "        S_lst = net(X)\n",
    "        loss = loss_func(net, X, S_lst)\n",
    "        loss.backward()\n",
    "        history.add_scalar('loss', loss.data)\n",
    "        for l in range(net.depth - 1):\n",
    "            A = net.lsqnonneglst[l].A\n",
    "            # record history\n",
    "#             history.add_tensor('A'+str(l+1), A.data)\n",
    "#             history.add_tensor('grad_A'+str(l+1), A.grad.data)\n",
    "#             history.add_tensor('S' + str(l+1), S_lst[l].data)\n",
    "            # projection gradient descent\n",
    "            A.data = A.data.sub_(lr*A.grad.data) # ---> A.data = A.data - lr*A.grad.data\n",
    "            A.data = A.data.clamp(min = 0)\n",
    "        lr = lr*weight_decay\n",
    "        if (i+1)%10 == 0:\n",
    "            print('epoch = ', i+1, '\\n', loss.data)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(net, X, loss_func, label, L= None, epoch = 10, lr_nmf = 1e-3, lr_classification = 1e-3, weight_decay = 1):\n",
    "    '''\n",
    "    ---- Description\n",
    "    Training the supervised Deep_NMF with projection gradient descent. Details for the training process:\n",
    "        for each epoch we update the NMF layer and the classification layer separately. First update the NMF layer for once\n",
    "        and then update the classification layer for thirty times. The learning rate is \n",
    "    ---- Inputs:\n",
    "    net: A Deep_NMF object, note that it should be the unsupervised version, so c = None for the Deep_NMF.\n",
    "    X: The data matrix.\n",
    "    epoch: How many time you want to feed in the data matrix to the network, default 10\n",
    "    loss_func: The loss function, should be a Energy_Loss_func object\n",
    "    epoch: How many time you want to feed in the data matrix to the network, default 10\n",
    "    lr_nmf: the learning rate for the NMF layer\n",
    "    lr_classification: the learning rate for the classification layer\n",
    "    weight_decay: the weight decay parameter, doing lr = lr*weight_decay every epoch\n",
    "    '''\n",
    "    \n",
    "    history = Writer() # creating a Writer object to record the history for the training process\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        # doing gradient update for NMF layer\n",
    "        net.zero_grad()\n",
    "        S_lst, pred = net(X)\n",
    "        loss = loss_func(net, X, S_lst, pred, label, L)\n",
    "        loss.backward()\n",
    "        for l in range(net.depth - 1):\n",
    "            history.add_scalar('loss',loss.data)\n",
    "            A = net.lsqnonneglst[l].A\n",
    "            # record history\n",
    "            history.add_tensor('A'+str(l+1), A.data)\n",
    "            history.add_tensor('grad_A'+str(l+1), A.grad.data)\n",
    "            history.add_tensor('S' + str(l+1), S_lst[l].data)\n",
    "            # projection gradient descent\n",
    "            A.data = A.data.sub_(lr_nmf*A.grad.data)\n",
    "            A.data = A.data.clamp(min = 0)\n",
    "            \n",
    "        # doing gradient update for classification layer\n",
    "        for iter_classifier in range(50):\n",
    "            net.zero_grad()\n",
    "            for A in net.lsqnonneglst.parameters():\n",
    "                A.requires_grad = False\n",
    "            S_lst, pred = net(X)\n",
    "            loss = loss_func(net, X, S_lst, pred, label, L)\n",
    "            loss.backward()\n",
    "            S_lst[0].detach()\n",
    "            history.add_scalar('loss',loss.data)\n",
    "            weight = net.linear.weight\n",
    "            weight.data = weight.data.sub_(lr_classification*weight.grad.data)\n",
    "            history.add_tensor('weight', weight.data.clone())\n",
    "            history.add_tensor('grad_weight', weight.grad.data.clone())\n",
    "        for A in net.lsqnonneglst.parameters():\n",
    "                A.requires_grad = True\n",
    "        \n",
    "        \n",
    "        lr_nmf = lr_nmf*weight_decay\n",
    "        lr_classification = lr_classification*weight_decay\n",
    "        \n",
    "        print('epoch = ', i+1, '\\n', loss.data)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
