{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How I get the 20 news groups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import Ipynb_importer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "data = fetch_20newsgroups_vectorized(subset = 'all')\n",
    "\n",
    "\n",
    "target = data.target\n",
    "target_names = data.target_names\n",
    "data = data.data\n",
    "\n",
    "save_npz('20news', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import Ipynb_importer\n",
    "from deep_nmf import Deep_NMF, Energy_Loss_Func\n",
    "from writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matrix is tooo large if transformed into a torch tensor, so I have to do that in a stochastic way.\n",
    "# Here I define a dataloader that will take the subset of the sparse numpy matrix and then only transform this subset into\n",
    "# a torch Tensor, which will save memories\n",
    "\n",
    "# class sparsedata(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data, label,transform = None):\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "#         self.transform = transform\n",
    "#         self.len = data.shape[0]\n",
    "#         assert(self.len == len(label))\n",
    "#     def __getitem__(self, index):\n",
    "#         inputs = self.data[index,:]\n",
    "#         if self.transform is not None:\n",
    "#             inputs = self.transform(inputs)\n",
    "#         target = self.label[index]\n",
    "#         inputs = torch.Tensor(inputs.todense()).double()\n",
    "#         if type(index) == int:\n",
    "#             target = torch.Tensor([target])\n",
    "#         else:\n",
    "#             target = torch.Tensor(target)\n",
    "#         return inputs, target\n",
    "#     def __len__(self):\n",
    "#         return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another dataset class for loading the semisupervised version for crossentropy criterion\n",
    "class sparsedata_cr_entr(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label, l = None, transform = None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.len = data.shape[0]\n",
    "        self.l = l\n",
    "        assert(self.len == len(label))\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.data[index,:]\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "        target = self.label[index]\n",
    "        inputs = torch.Tensor(inputs.todense()).double()\n",
    "        if type(index) == int:\n",
    "            target = torch.Tensor([target]).long()\n",
    "        else:\n",
    "            target = torch.Tensor(target).long()\n",
    "        if self.l is None:\n",
    "            return inputs, target\n",
    "        else:\n",
    "            l = self.l[index,:]\n",
    "            if type(index) == int:\n",
    "                l = torch.Tensor([l]).double()\n",
    "            else:\n",
    "                l = torch.Tensor(l)\n",
    "            return inputs, target, l.double()\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dataset class for loading the semisupervised version for L2 criterion\n",
    "class sparsedata_L2(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, Y, L = None, transform = None):\n",
    "        self.data = data\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "        self.len = data.shape[0]\n",
    "        self.L = L\n",
    "        if L is not None:\n",
    "            assert(self.len == L.shape[0])\n",
    "        assert(self.len == Y.shape[0])\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.data[index,:]\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "        target = self.Y[index,:]\n",
    "        inputs = torch.Tensor(inputs.todense()).double()\n",
    "        if type(index) == int:\n",
    "            target = torch.Tensor([target]).double()\n",
    "        else:\n",
    "            target = torch.Tensor(target).double()\n",
    "        if self.L is None:\n",
    "            return inputs, target\n",
    "        else:\n",
    "            L = self.L[index]\n",
    "            if type(index) == int:\n",
    "                L = torch.Tensor([L]).double()\n",
    "            else:\n",
    "                L = torch.Tensor(L).double()\n",
    "            return inputs, target, L\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the dataset class\n",
    "import scipy.io as sio\n",
    "directory = 'C:/Users/rzhang/Dropbox/Deep NMF/Code/Eli code/20_news/known_labels'\n",
    "L = sio.loadmat(directory)\n",
    "L20 = L.get('L20').T\n",
    "L50 = L.get('L50').T\n",
    "L90 = L.get('L90').T\n",
    "l = L20[:,0]\n",
    "directory_Y = 'C:/Users/rzhang/Dropbox/Deep NMF/Code/Eli code/20_news/Y'\n",
    "Y = sio.loadmat(directory_Y)\n",
    "Y = Y.get('Y').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network structure\n",
    "m = data.shape[1]\n",
    "k1 = 200\n",
    "k2 = 20\n",
    "net = Deep_NMF([m,k2],20)\n",
    "loss_func = Energy_Loss_Func(lambd = 1, classification_type = 'L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training process!\n",
    "\n",
    "# # setting training parameters\n",
    "# epoch = 5\n",
    "# lr = 10\n",
    "# loss_lst = []\n",
    "# # train!\n",
    "# for epo in range(epoch):\n",
    "#     dataloader = torch.utils.data.DataLoader(dataset, batch_size = 5, shuffle = True)\n",
    "#     total_loss = 0\n",
    "#     for (i, (inputs, label,l_batch)) in enumerate(dataloader):\n",
    "#         t1 = time.time()\n",
    "#         inputs = inputs.view([inputs.shape[0], inputs.shape[2]])\n",
    "#         inputs, label = Variable(inputs), Variable(label)\n",
    "#         S_lst,pred = net(inputs)\n",
    "#         loss = loss_func(net, inputs, S_lst,pred,label.view([label.shape[0], -1]),l_batch.view([l_batch.shape[0], -1]))\n",
    "#         loss.backward()\n",
    "#         loss_lst.append(loss.data)\n",
    "#         t2 = time.time()\n",
    "#         total_loss += loss.data\n",
    "#         print(loss.data)\n",
    "#         for A in net.parameters():\n",
    "#             A.data = A.data.sub_(lr*A.grad.data)\n",
    "#             A.data = A.data.clamp(min = 0)\n",
    "#     print('epoch = ', epo, '\\n', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whole_output(net, dataset, param_lst = None):\n",
    "    history = Writer()\n",
    "    # initialize the network with certain initial value\n",
    "    if param_lst is not None:\n",
    "        for (i,param) in enumerate(net.parameters()):\n",
    "            param.data = param_lst[i]\n",
    "    # start to forward propagate, 100 at a time\n",
    "    n = len(dataset)\n",
    "    if n%100 == 0:\n",
    "        batch_num = n/100\n",
    "    else:\n",
    "        batch_num = n//100 + 1\n",
    "    print('batch_num = ', batch_num, '\\n')\n",
    "    for i in range(batch_num):\n",
    "        print('current at batch:', i)\n",
    "        try:\n",
    "            (inputs, label) = dataset[i*100:(i+1)*100]\n",
    "        except:\n",
    "            (inputs, label) = dataset[i*100:]\n",
    "        history.add_tensor('label', label)\n",
    "        output = net(inputs)\n",
    "        history.add_tensor('output', output)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
