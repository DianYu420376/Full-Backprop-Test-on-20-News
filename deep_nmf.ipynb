{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date:2018.07.21\n",
    "# Author: Runyu Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from lsqnonneg_module.ipynb\n",
      "None\n",
      "tensor([[-1.0074,  0.3631, -0.3648, -1.6411, -0.7826, -1.9915, -1.1896,\n",
      "          0.0418, -1.7939, -0.4538],\n",
      "        [-0.5330, -0.3601, -0.4655, -0.0168,  0.0164, -0.0161, -0.4415,\n",
      "         -0.1296, -0.1100, -0.0057]], dtype=torch.float64)\n",
      "tensor([[-0.8921,  0.4996, -0.3140, -1.4765, -0.6871, -1.9745, -1.0919,\n",
      "          0.2329, -1.6408, -0.4500],\n",
      "        [-0.5330, -0.3601, -0.4655, -0.0168,  0.0164, -0.0161, -0.4415,\n",
      "         -0.1296, -0.1100, -0.0057]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.io as sio\n",
    "import Ipynb_importer\n",
    "from lsqnonneg_module import LsqNonneg\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_NMF(nn.Module):\n",
    "    '''\n",
    "    Build a Deep NMF network structure.\n",
    "    \n",
    "    initial parameters:\n",
    "    depth_info: list, [m, k1, k2,...k_L] # Note! m must be contained in the list, which is different from the matlab version\n",
    "    c: default None, otherwise it should be a scalar indicating how many classes there are\n",
    "    \n",
    "    the Deep_NMF object contains several NMF layers(contained in self.lsqnonneglst, each element in the list self.lsqnonneglst is a Lsqnonneg object)\n",
    "    and a linear layer for classification(self.linear).\n",
    "    '''\n",
    "    def __init__(self, depth_info, c = None):\n",
    "        super(Deep_NMF, self).__init__()\n",
    "        self.depth_info = depth_info\n",
    "        self.depth = len(depth_info)\n",
    "        self.c= c\n",
    "        self.lsqnonneglst = nn.ModuleList([LsqNonneg(depth_info[i], depth_info[i+1]) \n",
    "                                           for i in range(self.depth-1)])\n",
    "        if c is not None:\n",
    "            self.linear = nn.Linear(depth_info[-1],c, bias = False).double()\n",
    "    def forward(self, X):\n",
    "        S_lst = []\n",
    "        for i in range(self.depth-1):\n",
    "            X = self.lsqnonneglst[i](X)\n",
    "            S_lst.append(X)\n",
    "        if self.c is None:\n",
    "            return S_lst\n",
    "        else:\n",
    "            pred = self.linear(X)\n",
    "            return S_lst, pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Energy_Loss_Func(nn.Module):\n",
    "    '''\n",
    "    Defining the energy loss function as in the paper deep NMF\n",
    "    \n",
    "    initial parameter: \n",
    "        lambd: the regularization parameter, defining how important the classification error is.\n",
    "        classification_type: string, 'L2' or 'CrossEntropy'. Default 'CrossEntropy'\n",
    "    '''\n",
    "    def __init__(self,lambd = 0, classification_type = 'CrossEntropy'):\n",
    "        super(Energy_Loss_Func, self).__init__()\n",
    "        self.lambd = lambd\n",
    "        self.classification_type = classification_type\n",
    "        self.criterion1 = ReconstructionLoss()\n",
    "        if classification_type == 'L2':\n",
    "            self.criterion2 = ClassificationLossL2()\n",
    "        else:\n",
    "            self.criterion2 = ClassificationLossCrossEntropy()\n",
    "            \n",
    "    def forward(self, net, X, S_lst, pred = None, label = None, L = None):\n",
    "        total_reconstructionloss = self.criterion1(X, S_lst[0], net.lsqnonneglst[0].A)\n",
    "        depth = net.depth\n",
    "        for i in range(1,depth-1):\n",
    "            total_reconstructionloss += self.criterion1(S_lst[i-1], S_lst[i], net.lsqnonneglst[i].A)\n",
    "        if pred is None:\n",
    "            # unsupervised case\n",
    "            assert(label is None and L is None)\n",
    "            return total_reconstructionloss\n",
    "        else:\n",
    "            # fully supervised case and semisupervised case\n",
    "            classificationloss = self.criterion2(pred, label, L)\n",
    "            return total_reconstructionloss + self.lambd*classificationloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining all kinds of loss functions that is needed\n",
    "\n",
    "class Fro_Norm(nn.Module):\n",
    "    '''\n",
    "    calculate the Frobenius norm between two matrices of the same size.\n",
    "    Do: criterion = Fro_Norm()\n",
    "        loss = criterion(X1,X2) and the loss is the entrywise average of the square of Frobenius norm.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Fro_Norm, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "    def forward(self,X1, X2):\n",
    "        len1 = torch.numel(X1.data)\n",
    "        len2 = torch.numel(X2.data)\n",
    "        assert(len1 == len2)\n",
    "        X = X1 - X2\n",
    "        #X.contiguous()\n",
    "        return self.criterion(X.view(len1), Variable(torch.zeros(len1).double()))\n",
    "\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    '''\n",
    "    calculate the reconstruction error ||X - AS||_F^2.\n",
    "    Do: criterion = ReconstructionLoss()\n",
    "        loss = criterion(X, S, A) and the loss is the entrywise average of the square of Frobenius norm ||X - AS||_F^2.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "        self.criterion = Fro_Norm()\n",
    "    def forward(self, X, S, A):\n",
    "        X_approx = torch.mm(S,A)\n",
    "        reconstructionloss = self.criterion(X_approx, X)\n",
    "        return reconstructionloss\n",
    "\n",
    "class ClassificationLossL2(nn.Module):\n",
    "    '''\n",
    "    calculate the classification loss, using the criterion ||L.*(Y - Y_pred)||_F^2.\n",
    "    Do: criterion = ReconstructionLoss()\n",
    "        loss = criterion(Y, Y_pred) and the loss is the entrywise average of the square of Frobenius norm ||Y - Y_pred||_F^2.\n",
    "        loss = criterion(Y, Y_pred, L) and the loss is the entrywise average of the square of the Frobenius norm ||L.*(Y - Y_pred)||_F^2\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ClassificationLossL2, self).__init__()\n",
    "        self.criterion = Fro_Norm()\n",
    "    def forward(self, Y, Y_pred, L = None):\n",
    "        if L is None:\n",
    "            classificationloss = self.criterion(Y_pred, Y)\n",
    "            return classificationloss\n",
    "        else:\n",
    "            classificationloss = self.criterion(L*Y_pred, L*Y)\n",
    "            return classificationloss\n",
    "\n",
    "class ClassificationLossCrossEntropy(nn.Module):\n",
    "    '''\n",
    "    calculate the classification loss, using the criterion ||L.*(Y - Y_pred)||_F^2.\n",
    "    Do: criterion = ReconstructionLoss()\n",
    "        loss = criterion(Y, Y_pred) and the loss is the entrywise average of the square of Frobenius norm ||Y - Y_pred||_F^2.\n",
    "        loss = criterion(Y, Y_pred, L) and the loss is the entrywise average of the square of the Frobenius norm ||L.*(Y - Y_pred)||_F^2\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ClassificationLossCrossEntropy, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    def forward(self, Y_pred, label, L = None):\n",
    "        if L is None:\n",
    "            classificationloss = self.criterion(Y_pred, label)\n",
    "            return classificationloss\n",
    "        else:\n",
    "            l = Variable(L[:,0].data.long())\n",
    "            classificationloss = self.criterion(L*Y_pred, l*label)\n",
    "            return classificationloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
